{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¾ Tennis Match Analysis with Autodistill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Project Goal  \n",
    "\n",
    "The goal of this project is to create a dataset in a fully automated way, which will be used to train computer vision models. These models will form part of a pipeline designed to process videos of tennis matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ” Pipeline Overview  \n",
    "The pipeline used to analyze video frames consists of the following five components:\n",
    "\n",
    "1. **Frame Classification**  \n",
    "2. **Player Detector**  \n",
    "3. **Court Landmarks Detector**  \n",
    "4. **Homography Matrix Computation**  \n",
    "5. **Obtain Players' (x, y) Position on the Minimap**\n",
    "\n",
    "In the image below, you can see the full pipeline scheme.\n",
    "\n",
    "![Pipeline Overview](assets/pipeline_overview.png)\n",
    "\n",
    "This notebook will cover the following parts:\n",
    "* Download the dataset  \n",
    "* Extract frames from the videos  \n",
    "* Frame Classification  \n",
    "    * Dataset creation  \n",
    "    * Model training  \n",
    "* Player Detector  \n",
    "    * Dataset creation  \n",
    "    * Model training  \n",
    "* Court Landmarks Detector  \n",
    "    * Dataset creation  \n",
    "    * Model training  \n",
    "* Homography Matrix Computation  \n",
    "* Obtain Players' (x, y) Position on the Minimap  \n",
    "\n",
    "ðŸš€ **Let's get started and enjoy the process!** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Download the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by running the `download_dataset.sh` script to download the videos. This script will download the videos from the provided list of links and save them in the `dataset/videos` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Run the script\n",
    "!bash download_dataset.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the path to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dir_dataset = Path(\"dataset\")\n",
    "dir_dataset_videos = dir_dataset / \"videos\"\n",
    "\n",
    "lst_video_paths = list(dir_dataset_videos.glob(\"**/*.mp4\"))\n",
    "print(f\"Dataset contains {len(lst_video_paths)} videos:\")\n",
    "for video_path in lst_video_paths:\n",
    "    print(str(video_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¥ Extract frames from the videos  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the frames from the videos. This will create a new folder called `dataset/frames` with the frames extracted from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_autodistillation.utils.video import split_video_to_frames\n",
    "\n",
    "# Define the path to the extracted frames\n",
    "dir_dataset_frames = dir_dataset / \"frames\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Split the videos into frames\n",
    "for video_path in lst_video_paths:\n",
    "    split_video_to_frames(\n",
    "        video_path=video_path,\n",
    "        output_folder = dir_dataset_frames,\n",
    "        frames_per_second=1,\n",
    "        overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_extracted_frames = list(dir_dataset_frames.glob(\"**/*.jpg\"))\n",
    "total_extracted_frames = len(lst_extracted_frames)\n",
    "print(f\"Total extracted frames: {total_extracted_frames}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the extracted frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "import random\n",
    "SAMPLE_SIZE = 16\n",
    "SAMPLE_GRID_SIZE = (4, 4)\n",
    "SAMPLE_PLOT_SIZE = (16, 10)\n",
    "\n",
    "selected_frames = random.sample(lst_extracted_frames, SAMPLE_SIZE)\n",
    "titles = [\n",
    "    image_path.stem\n",
    "    for image_path\n",
    "    in selected_frames]\n",
    "images = [\n",
    "    cv2.imread(str(image_path))\n",
    "    for image_path\n",
    "    in selected_frames[:SAMPLE_SIZE]]\n",
    "\n",
    "sv.plot_images_grid(images=images, titles=titles, grid_size=SAMPLE_GRID_SIZE, size=SAMPLE_PLOT_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â—**Notice** that not all frames contain a full tennis court showing a clear view of the players and the court. Some frames are just commercials, close-ups or even details of the court, that usually in a tennis match, it represents a replay.\n",
    "\n",
    "The next step is to \"automagically\" ðŸ§™ annotate frames that display a full tennis court with a clear view of both the players and the court. This will allow us to train a model capable of distinguishing between different scenarios, such as commercials, close-ups, partial court views, and actual gameplay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Frame Classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will use chatGPT to annotate the frames we extracted. This will allow us to train a model capable of distinguishing between different scenarios.\n",
    "\n",
    "We will start by separating the frames by court types and split them into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Dataset creation (frame classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's separate the frames by court types. Note that the name of each frame or video starts with a court type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_videos_court_type = {}\n",
    "for video_path in lst_video_paths:\n",
    "    court_type = video_path.stem.split(\"-\")[0]\n",
    "    dict_videos_court_type.setdefault(court_type, []).append(video_path.resolve())\n",
    "\n",
    "dict_videos_court_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the proportions of videos to be used for training and testing sets, considering the different types of courts. \n",
    "\n",
    "We want, for instance, 80% of the videos with \"grass\" court type will be used for training and 20% for validation. Simillary, 80% of the videos with \"red clay\" court type will be used for training and 20% for validation.\n",
    "\n",
    "As each video has a different number of frames, the amount of frames to be used for training and testing will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2026)\n",
    "\n",
    "# Proportions of training and validation sets\n",
    "prop_train = 0.8 #  80% into training set\n",
    "prop_val = 0.2 #  20% into validation set\n",
    "\n",
    "videos_split = {\"train\": [], \"val\": []}\n",
    "# For each court type, separate videos into training and testing sets\n",
    "for court_type, videos in dict_videos_court_type.items():\n",
    "    # Shuffle videos\n",
    "    random.shuffle(videos)\n",
    "    # Get the number of videos to use for training and validation\n",
    "    num_train = int(len(videos) * prop_train)\n",
    "    # Separate videos into training and validation sets\n",
    "    videos_split[\"train\"].extend(videos[:num_train])\n",
    "    videos_split[\"val\"].extend(videos[num_train:])\n",
    "\n",
    "videos_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now separate the frames in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_frames_split = {}\n",
    "\n",
    "# Get a list of frames in each split\n",
    "for split_name, lst_videos in videos_split.items():\n",
    "    # Create the split folder\n",
    "    dir_split = dir_dataset_frames / split_name\n",
    "    dir_split.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for video_path in lst_videos:\n",
    "        video_name = video_path.stem\n",
    "        lst_frames_split = list(dir_dataset_frames.glob(f\"{video_name}*.jpg\"))\n",
    "\n",
    "        # Move the files to the split folder\n",
    "        for frame_path in lst_frames_split:\n",
    "            frame_path.rename(dir_split / frame_path.name)\n",
    "\n",
    "        dict_frames_split.setdefault(split_name, []).extend(lst_frames_split)\n",
    "    print(f\"Number of frames in {split_name}: {len(dict_frames_split[split_name])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will use chatGPT to annotate the frames we extracted. We want chatGPT to annotate the frames with the following ontology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ontology = {\n",
    "            \"an image showing the full playable area of a tennis court with camera positioned higher and centered, providing a full view of the entire court\": \"game_play\",\n",
    "            \"an image showing partially the playable area of a tennis court, providing a partial view of the court\": \"partial_view\",\n",
    "            \"a close-up shot of a tennis player\": \"close_up\",\n",
    "            \"anything else\": \"ignore1\",\n",
    "            \"none\": \"ignore2\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which will make our classification model to classify each frame in one of the following classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_return = [f\"* {v}\" for v in dict_ontology]\n",
    "classes_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, we will pass the following prompt to chatGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"What is in the best description of the image?\n",
    "Return one of these options that best describes the image:\n",
    "{'\\n'.join(classes_to_return)}\n",
    "You MUST return only one of the options from that list. Nothing besides that.\n",
    "\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the `GPT4V` class to annotate the frames.\n",
    "\n",
    "Don't forget to set your OpenAI API key in the .env file.  \n",
    "\n",
    "For example:\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill.detection import CaptionOntology\n",
    "from autodistill_gpt_4v import GPT4V\n",
    "\n",
    "gpt4v = GPT4V(\n",
    "    ontology=CaptionOntology(\n",
    "        dict_ontology\n",
    "    ),\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    resize_min=360,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make calls to ChatGPT API so it can annotate our frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - A shelve file is used to store the results of the annotations\n",
    "shelve_file = dir_dataset / \"frames_prediction_v0.db\"\n",
    "\n",
    "# Define the output folder for the annotations\n",
    "dir_output_annotations_gpt4v = dir_dataset / f\"frames_labeled_by_gpt4v\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def post_process(result):\n",
    "    result = result.lower()\n",
    "    result = result.replace(\"*\", \"\")\n",
    "    result = result.replace(\"'\", \"\")\n",
    "    result = result.strip()\n",
    "    return result\n",
    "\n",
    "# For each split, call ChatGPT to annotate the frames\n",
    "for split_name in [\"train\", \"val\"]:\n",
    "    dir_split = dir_dataset_frames / split_name\n",
    "    dir_output_annotatations = dir_output_annotations_gpt4v / split_name\n",
    "    # Use autodistill to label the frames\n",
    "    dataset = gpt4v.label(dir_split.resolve(), extension=\".jpg\",\n",
    "                output_folder=dir_output_annotatations.resolve(),\n",
    "                fn_post_process=post_process,\n",
    "                shelve_file=Path(shelve_file).resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "lst_output_annotations_gpt4v = list(dir_output_annotations_gpt4v.glob(\"**/*.jpg\"))\n",
    "\n",
    "# Randomly sample SAMPLE_SIZE images\n",
    "lst_selected_samples = random.sample(lst_output_annotations_gpt4v, SAMPLE_SIZE)\n",
    "# Get the titles based on their folder name\n",
    "titles = [image_path.parts[-2] for image_path in lst_selected_samples]\n",
    "\n",
    "images = [\n",
    "    cv2.imread(str(image_path))\n",
    "    for image_path\n",
    "    in lst_selected_samples]\n",
    "\n",
    "sv.plot_images_grid(images=images, titles=titles, grid_size=SAMPLE_GRID_SIZE, size=SAMPLE_PLOT_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, huh? ðŸ¤©\n",
    "\n",
    "Now, let's train a model with those samples, so that the model can learn how to classify the frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Model training (frame classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the images annotated by ChatGPT with autodistill to train a YOLOv11 nano model, which is a very small model that can be used for real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Import the YOLOv11 classification model and train it\n",
    "from autodistill_yolov11 import YOLOv11\n",
    "\n",
    "yolo_classification_model = YOLOv11(\"yolo11n-cls.pt\")\n",
    "# We unable erasing, mosaic and crop, because the samples of game_play and partial_view would be very similar\n",
    "yolo_classification_model.train(dir_output_annotations_gpt4v, epochs=200, device=0, erasing=0, mosaic=0, crop_fraction=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the annotations from ChatGPT, we can measure the accuracy of our trained model to see how close to ChatGPT's annotations it is.\n",
    "\n",
    "We will save the images classified by YOLO for further visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load the classification model\n",
    "yolo_classification_model = YOLO(\"runs/classify/train/weights/best.pt\")\n",
    "# Define a folder where images will be saved\n",
    "dir_classified_frames = dir_dataset / \"frames_classified_by_yolo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPSILON = 1e-9\n",
    "\n",
    "results = {}\n",
    "for split in [\"train\", \"val\"]:\n",
    "    results.setdefault(split, {})\n",
    "    print(f\"\\nðŸ” Results for {split}:\")\n",
    "    dir_split = dir_output_annotations_gpt4v / split\n",
    "    for target_class_dir in dir_split.glob(\"*\"):\n",
    "        target_class_name = target_class_dir.stem\n",
    "        results[split].setdefault(target_class_name, [])\n",
    "\n",
    "        # Get all samples in the target class\n",
    "        lst_samples = list(target_class_dir.glob(\"*.jpg\"))\n",
    "\n",
    "        # Run the model on all samples\n",
    "        pbar = tqdm(lst_samples, total=len(lst_samples))\n",
    "        for img_path in pbar:\n",
    "            pbar.set_description(f\"ðŸ”¥ Running model on {img_path.stem}\")\n",
    "            pred = yolo_classification_model.predict(img_path, verbose=False)\n",
    "            class_name = yolo_classification_model.names[pred[0].probs.top1]\n",
    "            results[split][target_class_name].append(class_name == target_class_name)\n",
    "\n",
    "            # Copy the images to its folder following its prediction\n",
    "            dir_output_class = dir_classified_frames / target_class_name\n",
    "            dir_output_class.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy(img_path, dir_output_class / img_path.name)\n",
    "\n",
    "        total_correct = sum(results[split][target_class_name])\n",
    "        total_samples = len(results[split][target_class_name])\n",
    "        if total_samples == 0:\n",
    "            print(f\"âš ï¸ No samples for {split} '{target_class_name}'\")\n",
    "            continue\n",
    "        split_class_accuracy = total_correct / (total_samples + EPSILON)\n",
    "        print(f\"ðŸ“Š Accuracy for '{target_class_name}': {100 * split_class_accuracy:.2f}%\")\n",
    "\n",
    "    # Accuracy for the entire split\n",
    "    flat_results = list(chain.from_iterable(results[split].values()))\n",
    "    total_correct = sum(flat_results)\n",
    "    total_samples = len(flat_results)\n",
    "    total_accuracy = total_correct / (total_samples + EPSILON)\n",
    "    print(f\"âœ… Final accuracy for split '{split}': {100 * total_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to see some of the images that were classified as \"game_play\" by yolo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_classified_frames = list(dir_classified_frames.glob(\"**/*.jpg\"))\n",
    "\n",
    "# Randomly sample SAMPLE_SIZE images\n",
    "lst_selected_samples = random.sample(lst_classified_frames, SAMPLE_SIZE)\n",
    "# Get the titles based on their folder name\n",
    "titles = [image_path.parts[-2] for image_path in lst_selected_samples]\n",
    "\n",
    "images = [\n",
    "    cv2.imread(str(image_path))\n",
    "    for image_path\n",
    "    in lst_selected_samples\n",
    "]\n",
    "\n",
    "sv.plot_images_grid(images=images, titles=titles, grid_size=SAMPLE_GRID_SIZE, size=SAMPLE_PLOT_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SAM to annotate the frames "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the player detector, we will use Segment Anything Model (SAM) to retrieve the players' mask, which will be converted to a bounding box.\n",
    "\n",
    "SAM will also be used to retrieve the court mask, which will be used to  find the corners of the court, and then compute the homography matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to load grounding dino directly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill.detection import CaptionOntology\n",
    "\n",
    "# Let's define a directory where the SAM annotations will be saved\n",
    "dir_output_annotations_sam = dir_dataset / \"sam_labeled_data\"\n",
    "\n",
    "# Let's define the ontology for the SAM model:\n",
    "base_model_grounded_sam = GroundedSAM(\n",
    "    ontology=CaptionOntology(\n",
    "        {\"playable area of a tennis court\": \"playable_area\",\n",
    "         \"tennis player\": \"tennis_player\"}\n",
    "    ),\n",
    ")\n",
    "id2class = {i:class_info[1] for i, class_info in enumerate(base_model_grounded_sam.ontology.promptMap)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use SAM to annotate the frames classified as \"game_play\" by YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Run the SAM model to annotate the frames\n",
    "labeled_mask_dataset = base_model_grounded_sam.label(str(dir_classified_frames / \"game_play\"),\n",
    "                                                      extension=\".jpg\",\n",
    "                                                      output_folder=str(dir_output_annotations_sam),\n",
    "                                                      record_confidence=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load pre-computed SAM annotations\n",
    "import pickle\n",
    "labeled_mask_dataset = pickle.load(open(\"labeled_mask_dataset.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labeled_mask_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the masks predicted by SAM to create a dataset for the player detector and the keypoints detector.\n",
    "\n",
    "For the player detector, we will transform the masks to bounding boxes.\n",
    "\n",
    "ðŸ’ For the keypoints detector, we will transform the masks to keypoints to get the 4 corners of the playable area, and we will use the homography matrix to infer the position of the other 14 points of the court map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Using supervision to visualize the labeled dataset annotated with SAM\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mask_annotator \u001b[38;5;241m=\u001b[39m \u001b[43msv\u001b[49m\u001b[38;5;241m.\u001b[39mMaskAnnotator()\n\u001b[1;32m      3\u001b[0m box_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mBoxAnnotator()\n\u001b[1;32m      4\u001b[0m label_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mLabelAnnotator()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sv' is not defined"
     ]
    }
   ],
   "source": [
    "# Using supervision to visualize the labeled dataset annotated with SAM\n",
    "mask_annotator = sv.MaskAnnotator()\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will loop over the annotations performed by SAM. The goal is to remove bad annotations and prepare the dataset for player detector and landmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_autodistillation.utils.coords_court_map import COORDS_COURT_MAP\n",
    "\n",
    "# Create a dictionary to map the points to their ids\n",
    "points_ids2classes = {point_name:idx for idx, point_name in enumerate(COORDS_COURT_MAP)}\n",
    "\n",
    "THRESH_PLAYABLE_AREA = 0.97\n",
    "images = []\n",
    "image_names = []\n",
    "dict_imgs2kpts = {}\n",
    "# Dictionary to store the detections of the playable area for each image\n",
    "dict_xyxy_playable_area = {}\n",
    "# Dictionary to store the detections of the players for each image\n",
    "dict_imgs2players = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to get the upper left corner of a mask\n",
    "def get_upper_left_corner(mask, img_w, thresh_width=0.15):\n",
    "    # Get the rows that have more than 15% of the image width\n",
    "    rows_sum = np.sum(mask, axis=1)\n",
    "    valid_rows = np.where(rows_sum > thresh_width*img_w)[0]\n",
    "    first_row, last_row = valid_rows[0].item(), valid_rows[-1].item()\n",
    "    # On the first row, get the first and last columns that has a 1\n",
    "    first_row_first_col = np.where(mask[first_row] == 1)[0][0].item()\n",
    "    return (first_row_first_col, first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "from tennis_autodistillation.utils.transformations import transform_image, transform_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m img_h, img_w \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Get all predicted classes\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m [\u001b[43mid2class\u001b[49m[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m annotation\u001b[38;5;241m.\u001b[39mclass_id]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Filter 1: Ignore images that do not have 2 players and 1 playable area\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_classes\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtennis_player\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m predicted_classes\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayable_area\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id2class' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the tennis court map\n",
    "tennis_court_map = cv2.imread(\"tennis_court_full_map.png\")\n",
    "\n",
    "# Loop over the SAM annotations\n",
    "for i, (image_path, image, annotation) in enumerate(labeled_mask_dataset):\n",
    "    if not Path(image_path).exists():\n",
    "        print(f\"âš ï¸ Image {image_path} does not exist\")\n",
    "        continue\n",
    "\n",
    "    # Get image dimensions\n",
    "    img_h, img_w = image.shape[:2]\n",
    "\n",
    "    # Get all predicted classes\n",
    "    predicted_classes = [id2class[idx] for idx in annotation.class_id]\n",
    "\n",
    "    # Filter 1: Ignore images that do not have 2 players and 1 playable area\n",
    "    if predicted_classes.count(\"tennis_player\") != 2 or predicted_classes.count(\"playable_area\") != 1:\n",
    "        continue\n",
    "\n",
    "    # Filter 2: Ignore images that the playable area is touching the borders of the image\n",
    "    id_det_playable_area = predicted_classes.index(\"playable_area\")\n",
    "    xyxy_playable_area = annotation.xyxy[id_det_playable_area]\n",
    "    # Most left and most right points of the playable area\n",
    "    x_min, y_min, x_max, y_max = xyxy_playable_area\n",
    "    p_bottom_left = x_min, y_max\n",
    "    p_bottom_right = x_max, y_max\n",
    "    width_bbx = xyxy_playable_area[2] - xyxy_playable_area[0]\n",
    "    high_bbx = xyxy_playable_area[3] - xyxy_playable_area[1]\n",
    "    # Check if the playable area is touching the borders of the image (97% of the image)\n",
    "    if high_bbx/img_h > THRESH_PLAYABLE_AREA or width_bbx/img_w > THRESH_PLAYABLE_AREA:\n",
    "        continue\n",
    "\n",
    "    # Sometimes the mask is not a single connected component, so we need to get the largest connected component\n",
    "    # Get the mask of the playable area\n",
    "    mask_playable_area = annotation.mask[id_det_playable_area]\n",
    "    # Get the largest connected component of the playable area\n",
    "    mask_uint8 = annotation.mask[id_det_playable_area].astype(np.uint8) * 255\n",
    "    # Find connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_uint8, connectivity=8)\n",
    "    # Identify the largest component (excluding background)\n",
    "    largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "    # Create a mask for the largest component\n",
    "    largest_mask = 1*(labels == largest_label)\n",
    "\n",
    "    # Get the corners of the largest_mask (playable area)\n",
    "    p1 = get_upper_left_corner(largest_mask, img_w)\n",
    "    # flip horizontally to get the upper right corner\n",
    "    p2 = get_upper_left_corner(np.fliplr(largest_mask), img_w)\n",
    "    p2 = (img_w - p2[0], p2[1])\n",
    "    # flip vertically to get the lower left corner\n",
    "    p3 = get_upper_left_corner(np.flipud(largest_mask), img_w,0.30)\n",
    "    p3 = (p3[0], img_h - p3[1])\n",
    "    # flip horizontally and vertically to get the lower right corner\n",
    "    p4 = get_upper_left_corner(np.flipud(np.fliplr(largest_mask)), img_w, 0.30)\n",
    "    p4 = (img_w - p4[0], img_h - p4[1])\n",
    "\n",
    "    # Filter 3: Ignore images that the edges of the playable area are not aligned with the corners of the bounding box of the playable area\n",
    "    # Distance from the bottom left and bottom right corners of the playable area with the bounding box of the playable area\n",
    "    d1 = np.linalg.norm(np.array(p3) - np.array(p_bottom_left))\n",
    "    d2 = np.linalg.norm(np.array(p4) - np.array(p_bottom_right))\n",
    "    if d1 > 15 or d2 > 15:\n",
    "        continue\n",
    "\n",
    "    # Filter 4: Ignore images that the playable are has considerable holes\n",
    "    # Get the bbx of the playable area\n",
    "    true_coords = np.argwhere(mask_playable_area)\n",
    "    min_y, min_x = true_coords.min(axis=0)  # Top-left corner (y, x)\n",
    "    max_y, max_x = true_coords.max(axis=0)  # Bottom-right corner (y, x)\n",
    "    # Merge the playable area mask with the players to remove the players\n",
    "    masks_players = [annotation.mask[i] for i, class_name in enumerate(predicted_classes) if class_name == \"tennis_player\"]\n",
    "    # Dilate the masks of the players\n",
    "    masks_players = [cv2.dilate(mask_player.astype(np.uint8), np.ones((15, 15))) for mask_player in masks_players]\n",
    "    mask_playable_area_players = mask_playable_area.copy()\n",
    "    for mask_player in masks_players:\n",
    "        mask_playable_area_players = np.logical_or(mask_playable_area_players, mask_player)\n",
    "    # Crop the merged playable area mask with players mask to the bbx of the playable area\n",
    "    mask_playable_area_players = mask_playable_area_players[min_y:max_y, min_x:max_x]\n",
    "    area_mask_playable_area_players = np.sum(mask_playable_area_players)\n",
    "    # Compute the theoritical area of the trapezoid (playable area)\n",
    "    trapezoid_area = ((p4[0]-p3[0]) + (p2[0]-p1[0])) * (mask_playable_area_players.shape[0]) / 2\n",
    "    # Check possible holes in the mask_playable_area_players\n",
    "    if area_mask_playable_area_players/trapezoid_area < THRESH_PLAYABLE_AREA:\n",
    "        continue\n",
    "\n",
    "    # Save the detections of the players in the dictionary\n",
    "    bbx_players = [annotation.xyxy[i] for i, class_name in enumerate(predicted_classes) if class_name == \"tennis_player\"]\n",
    "    # Make a dictionary with the player detections (player class id = 0)\n",
    "    dict_imgs2players[image_path] = sv.Detections(xyxy=np.array(bbx_players), class_id=np.zeros(len(bbx_players)))\n",
    "\n",
    "    # Let's compute homography matrix that maps p1, p2, p3, p4 to COORDS_COURT_MAP\n",
    "    src_pts = np.array([COORDS_COURT_MAP[\"p1\"], COORDS_COURT_MAP[\"p2\"], COORDS_COURT_MAP[\"p3\"], COORDS_COURT_MAP[\"p4\"]])\n",
    "    dst_pts = np.array([p1, p2, p3, p4])\n",
    "    H, _ = cv2.findHomography(src_pts, dst_pts)\n",
    "    # Project the image to the court map\n",
    "    transformed_image = transform_image(tennis_court_map, H, (img_w, img_h))\n",
    "    # Use H to transform all 14 points of the court map to the image\n",
    "    img_with_point = image.copy() # For drawing only\n",
    "    lst_keypoints, lst_class_ids = [], []\n",
    "    for point_name, point in COORDS_COURT_MAP.items():\n",
    "        transformed_point = transform_point(H, point)\n",
    "        transformed_point = tuple(map(int, transformed_point))\n",
    "        lst_keypoints.append(transformed_point)\n",
    "        lst_class_ids.append(points_ids2classes[point_name])\n",
    "\n",
    "    # Transform the list of keypoints to a (n,1,2) array\n",
    "    n = len(lst_keypoints)  # Number of detected objects\n",
    "    xy = np.array(lst_keypoints).reshape(n, 1, 2)\n",
    "    confidences = np.ones((n, 1))\n",
    "    class_ids = np.array(lst_class_ids)\n",
    "\n",
    "    # Store the keypoints\n",
    "    dict_imgs2kpts[image_path] = sv.KeyPoints(xy, class_ids, confidences)\n",
    "    # Store the detections of the playable area\n",
    "    dict_xyxy_playable_area[image_path] = tuple(map(float, xyxy_playable_area))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of images in each dictionary required to train the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_imgs2kpts), len(dict_xyxy_playable_area), len(dict_imgs2players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Dataset creation (player detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Let's create a dataset for the detections of the players obtained with SAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_output_players = dir_dataset / \"players\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for the player detector\n",
    "dataset_players = sv.DetectionDataset([\"player\"],\n",
    "                                      images=list(dict_imgs2players.keys()),\n",
    "                                      annotations=dict_imgs2players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervision.dataset.formats.yolo import save_yaml_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation and save them  (folder dataset/player will be created)\n",
    "ds_train, ds_val = dataset_players.split(split_ratio=0.8)\n",
    "dir_images_train = dir_output_players / \"train\" / \"images\"\n",
    "dir_labels_train = dir_output_players / \"train\" / \"labels\"\n",
    "ds_train.as_yolo(images_directory_path = dir_images_train, annotations_directory_path = dir_labels_train)\n",
    "dir_images_val = dir_output_players / \"val\" / \"images\"\n",
    "dir_labels_val = dir_output_players / \"val\" / \"labels\"\n",
    "ds_val.as_yolo(images_directory_path = dir_images_val, annotations_directory_path = dir_labels_val)\n",
    "data_yaml_path = dir_output_players / \"data.yaml\"\n",
    "# Save the data.yaml file\n",
    "data = {\"path\": str(dir_output_players.resolve()),\n",
    "        \"train\": \"train\",\n",
    "        \"val\": \"val\",\n",
    "        \"names\": dataset_players.classes}\n",
    "save_yaml_file(data=data, file_path=data_yaml_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Model training (player detector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_yolov11 import YOLOv11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Train the player detector\n",
    "yolo_player_model = YOLOv11(\"yolo11n.pt\")\n",
    "yolo_player_model.train(data_yaml_path, epochs=200, device=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the player detections looking at the results in the `runs/detect/train` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Court Key Points Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Dataset creation (keypoints detector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_autodistillation.utils.yolo import dataset_keypoints_to_yolo\n",
    "\n",
    "dir_output_keypoints = dir_dataset / \"court_keypoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset for the keypoints (folder dataset/court_keypoints will be created)\n",
    "dataset_keypoints = sv.DetectionDataset(list(dict_imgs2kpts.keys()),\n",
    "                                        list(dict_imgs2kpts.keys()),\n",
    "                                        dict_imgs2kpts)\n",
    "dataset_keypoints_to_yolo(\n",
    "    dataset_keypoints=dataset_keypoints,\n",
    "    dict_keypoints_classes=points_ids2classes,\n",
    "    dataset_bbxes=dict_xyxy_playable_area,\n",
    "    dir_output=dir_output_keypoints,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Model training (keypoints detector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Train the keypoints detector\n",
    "yolo_keypoint_model = YOLOv11(\"yolo11n-pose.pt\")\n",
    "yolo_keypoint_model.train(dir_output_keypoints / \"data.yaml\", epochs=200, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the keypoints detected by the model looking at the results in the `runs/pose/train` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ‰ We have successfully trained our `frame classifier`, `player detector` and `keypoints detector`! ðŸŽ‰\n",
    "\n",
    "You can use the output `.pt` generated with this notebook twith the `scripts/run_pipeline.py` and run inferences in your videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis-autodistill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
